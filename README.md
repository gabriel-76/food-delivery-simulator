# Food Delivery Simulator

## Vis√£o Geral

O **Food Delivery Simulator** √© um simulador de entrega de comida desenvolvido utilizando a biblioteca de simula√ß√£o de eventos discretos **SimPy**. O simulador foi adaptado para funcionar como um ambiente compat√≠vel com **Gymnasium**, permitindo experimentos com **Aprendizado por Refor√ßo**. O objetivo √© testar e treinar agentes utilizando algoritmos como **PPO (Proximal Policy Optimization)** da biblioteca **Stable-Baselines3**.

![Simulador de Delivery de Comida](simulator.gif)

## Requisitos

- Python 3.6 ou superior
- Gymnasium
- SimPy
- Stable-Baselines3
- Outras depend√™ncias listadas em `requirements.txt`

## Configura√ß√£o do Ambiente

### 1Ô∏è‚É£ Criar e ativar o ambiente virtual do Python

#### No Windows:
```shell
python -m venv venv
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
python -m venv venv
source venv/bin/activate
```

### 2Ô∏è‚É£ Instalar as depend√™ncias
```shell
python -m pip install -r requirements.txt
```

### 3Ô∏è‚É£ (Apenas no Linux) Instalar `python-tk` para usar o Matplotlib
```shell
sudo apt-get install python3-tk
```

## Uso do Simulador

### üîπ Sempre que for usar o script Python, ative o ambiente virtual:

#### No Windows:
```shell
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
source venv/bin/activate
```

### üîπ Rodar o script do teste do simulador:
```shell
python -m src.examples.test
```

## Treinamento de Agentes de Aprendizado por Refor√ßo

Para treinar um agente PPO com o ambiente do simulador. √â necess√°rio primeiramente montar um cen√°rio experimental definindo o valor dessas constantes:

```python
NUM_DRIVERS = 10
NUM_ORDERS = 12*24 # 12 pedidos por hora durante 24 horas
NUM_ESTABLISHMENTS = 10
NUM_COSTUMERS = NUM_ORDERS
GRID_MAP_SIZE = 50 # Tamanho do grid 50x50
REWARD_OBJECTIVE = 1
MAX_TIME_STEP = 60*24*2 # 2 dias
# 2 pedidos de 10 em 10 minutos
FUNCTION = lambda time: 2
LAMBDA_CODE = "lambda time: 2"
TIME_SHIFT = 10

# Vari√°veis para cria√ß√£o dos Motoristas
VEL_DRIVERS = [3, 5]

# Vari√°veis para cria√ß√£o dos Estabelecimentos
PREPARE_TIME = [20, 60]
OPERATING_RADIUS = [5, 30]
PRODUCTION_CAPACITY = [4, 4]

# Vari√°vel que controla quando o motorista deve ser alocado
# A porcentagem se refere ao progresso de prepara√ß√£o do pedido
# Exemplo: 0.7 indica que o motorista deve ser√° alocado quando o pedido estiver 70% pronto
PERCENTAGE_ALLOCATION_DRIVER = 0.7

NORMALIZE = True
```

Tendo em mente que a constante `REWARD_OBJECTIVE` define como as recompensas ser√£o calculadas. O valores poss√≠veis s√£o `1` para **Minimizar o tempo de entrega** e `2` para **Minimizar o custo de opera√ß√£o (dist√¢ncia)**.

Crie ent√£o o ambiente gymnasium de simula√ß√£o de entrega de comida a partir das constantes definidas e realize o treinamento:


```python
from stable_baselines3 import PPO
from simulator_env import FoodDeliveryEnv

# Criar o ambiente
env = FoodDeliveryGymEnv(
    num_drivers=NUM_DRIVERS,
    num_establishments=NUM_ESTABLISHMENTS,
    num_orders=NUM_ORDERS,
    num_costumers=NUM_COSTUMERS,
    grid_map_size=GRID_MAP_SIZE,
    vel_drivers=VEL_DRIVERS,
    prepare_time=PREPARE_TIME,
    operating_radius=OPERATING_RADIUS,
    production_capacity=PRODUCTION_CAPACITY,
    percentage_allocation_driver=PERCENTAGE_ALLOCATION_DRIVER,
    use_estimate=True,
    desconsider_capacity=True,
    max_time_step=MAX_TIME_STEP,
    reward_objective=REWARD_OBJECTIVE,
    function=FUNCTION,
    lambda_code=LAMBDA_CODE,
    time_shift=TIME_SHIFT,
    normalize=NORMALIZE,
    render_mode='human'
)

# Criar e treinar o modelo PPO
model = PPO("MultiInputPolicy", env, verbose=1, tensorboard_log="ppo_tensorboard/")
model.learn(total_timesteps=10000)

# Salvar o modelo treinado
model.save("ppo_food_delivery")
```

### Treinamento do Agentes RL a partir dos scripts prontos

#### 1Ô∏è‚É£ Script `train_ppo_model`
Utilize o script `train_ppo_model` localizado na pasta `src.examples` para realizar o treinamento do agente RL. Neste arquivo as constantes do cen√°rio experimental j√° foram definidas, mas fique a vontade para alter√°-las conforme seus objetivos.

Defina um caminho de um diret√≥rio a sua escolha para o salvamento dos modelos, logs e gr√°ficos do treinamento:

```python
RESULTS_DIR = "C:/Users/marco/OneDrive/√Årea de Trabalho/teste/"
```

Defina o n√∫mero `total_timesteps` e os outros hiperpar√¢metros conforme seus objetivos:

```python
model.learn(total_timesteps=10000, callback=eval_callback)
```

#### 2Ô∏è‚É£ Script `run_optimizer`

Utilize o script `run_optimizer` localizado na pasta `src.examples` para realizar multiplas execu√ß√µes dos agentes sobre o ambiente gymnasium e analisar as estatisticas gerais. Os Agentes dispon√≠veis s√£o: **Aleat√≥rio**, **Primeiro Motorista**, **Motorista mais Perto**, **Fun√ß√£o de Custo** e o **Agente de RL**.

Neste arquivo as constantes do cen√°rio experimental j√° foram definidas, mas fique a vontade para alter√°-las conforme seus objetivos.

Defina um caminho de um diret√≥rio a sua escolha para o salvamento dos modelos, logs e gr√°ficos do treinamento:

```python
RESULTS_DIR = "C:/Users/marco/OneDrive/√Årea de Trabalho/teste/"
```

Para escolher qual agente utilizar basta comentar o agente selecionado anteriormente e descomentar o agente desejado:

```python
#optimizer = RandomDriverOptimizerGym(gym_env)
#optimizer = FirstDriverOptimizerGym(gym_env)
#optimizer = NearestDriverOptimizerGym(gym_env)
#optimizer = LowestCostDriverOptimizerGym(gym_env, cost_function=SimpleCostFunction())
optimizer = RLModelOptimizerGym(gym_env, PPO.load("./best_model/best_model_6000000.zip"))
```

Defina tamb√©m a quantidade de execu√ß√µes:

```python
num_runs = 10
```

##### 1Ô∏è‚É£ Agente de Aprendizado Por Refor√ßo

Para rodar o agente de Aprendizado Por Refor√ßo √© necess√°rio primeiramente treinar um modelo e gerar um arquivo com o modelo em um arquivo `.zip`. √â necess√°rio passar o caminho do arquivo com o modelo como um par√¢metro da seguinte fun√ß√£o:

```python
optimizer = RLModelOptimizerGym(gym_env, PPO.load("./best_model/best_model_6000000.zip"))
```

##### 2Ô∏è‚É£ Agente Heur√≠stico da Fun√ß√£o de Custo

Para rodar o agente heur√≠stico da Fun√ß√£o de Custo √© necess√°rio primeiramente definir uma fun√ß√£o de custo em uma classe separada. A classe deve ser implementada a partir da interface `CostFunction`:

```python
class CostFunction(ABC):

    @abstractmethod
    def cost(self, map: Map, driver: Driver, route_segment: RouteSegment) -> Number:
        pass
```

##### 3Ô∏è‚É£ Adi√ß√£o de mais Agentes

√â poss√≠vel adicionar mais agentes para atuar sobre o ambiente. Para isso √© necess√°rio implementar um `Otimizador` a partir da interface `OptimizerGym` e sobreescrever as fun√ß√µes `select_driver` e `get_title`, assim como no exemplo abaixo:

```python
class RLModelOptimizerGym(OptimizerGym):

    def __init__(self, environment: FoodDeliveryGymEnv, model: PPO):
        super().__init__(environment)
        self.model = model

    def get_title(self):
        return "Otimizador por Aprendizado por Refor√ßo"

    def select_driver(self, obs: dict, drivers: List[Driver], route: Route):
        action, _states = self.model.predict(obs)
        return action
```